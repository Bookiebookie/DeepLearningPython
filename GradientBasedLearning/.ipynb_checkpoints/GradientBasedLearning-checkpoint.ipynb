{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS-EJ3311 - Deep Learning with Python, 09.09.2020-18.12.2020\n",
    "\n",
    "## Round 1 -   Gradient Based Optimization and Learning\n",
    "\n",
    "Shamsi Abdurakhmanova and Alexander Jung\n",
    "\n",
    "Aalto University, Espoo, Finland \n",
    "Fitech.io, Finland "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Goals\n",
    "\n",
    "- understand how to learn a predictor function by minimizing a loss function\n",
    "\n",
    "- understand how GD can be used to minimize loss function and to find optimal parameters of predictor\n",
    "\n",
    "- understand motivation for and implementation of SGD\n",
    "\n",
    "- understand the terms \"batch\", \"batch size\", \"learning rate\" and \"epochs\"\n",
    "\n",
    "- be aware of some variants for gradient-based optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Reading\n",
    "\n",
    "-  [chapter 2.4](https://livebook.manning.com/book/deep-learning-with-python/chapter-2/193) the book \"Deep Learning with Python\" by F. Chollet. \n",
    "\n",
    "## Additional Material (Optional!)\n",
    "\n",
    "### Hypothesis Space and Loss Functions\n",
    "\n",
    "- Video on Hypothesis Space https://youtu.be/CDcRfak1Mh4\n",
    "- Video on Loss Function https://youtu.be/Uv9lihDfsBs\n",
    "- Machine Learning: Basic Principles (Chapter 2 and 3) https://arxiv.org/pdf/1805.05052.pdf\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "- https://www.mathsisfun.com/calculus/derivatives-introduction.html\n",
    "- (chapters 3-4) https://openstax.org/books/calculus-volume-1/pages/3-1-defining-the-derivative#27277\n",
    "\n",
    "### Gradient Descent \n",
    "\n",
    "Video \n",
    "- Andrew Ng, https://www.youtube.com/watch?v=F6GSRDoB-Cg \n",
    "- StatQuest, https://www.youtube.com/watch?v=sDv4f4s2SB8\n",
    "- 3Blue1Brown, https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "\n",
    "Books\n",
    "\n",
    "- (intermediate) Machine Learning: Basic Principles (chapter 5) https://arxiv.org/pdf/1805.05052.pdf\n",
    "- (advanced) Deep Learning Book https://www.deeplearningbook.org/contents/optimization.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "A main component of many ML methods is a **hypothesis space**. The hypothesis space of a ML method is a subset of all possible **predictor maps**. A predictor map $h(\\mathbf{x})$ maps the feature vector $\\mathbf{x}$ of a data point to the predicted label $\\hat{y} = h(\\mathbf{x})$. The main goal of many ML methods is to find a good predictor map.\n",
    "\n",
    "For computational reasons, it is impossible to consider all possible such maps. Indeed, there are already uncountable infinite many maps that use a single real-valued feature $x$ and ouput a real-valued prediction $\\hat{y}$. Therefore, every ML method that runs on a physial computer, can only use a subset of maps which we refer to as the hypothesis space underlying that ML method. \n",
    "\n",
    "It is important to note that the hypothesis space is a design choice. One such choice for a hypothesis space are linear maps. Another choice for the hypothesis space is given by decision trees. The ML engineer has to choose the hypothesis space based on the available computational resources and the statistical properties of the data generated in the application at hand. \n",
    "\n",
    "The focus of this course on a particular class of ML methods, i.e., deep learning methods. Deep learning methods use hypothesis spaces that are parametrized by a weight (or parameter) vector \n",
    "\n",
    "$$ \\mathbf{w} = \\big(w_{1},\\ldots,w_{d}\\big)^{T}.$$ \n",
    "\n",
    "The entries $w_{i}$ of the weight vector are the individual weights associated with the connections (links) of an artificial neural network (see Round 2 for more details). A particular choice for the weight vector results in a predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$. Finding a good predictor is to find a weight vector such that the corresponding predictor $h^{(\\mathbf{w})}(\\mathbf{x})$ performs well. Maybe the most basic example of such a **parametrized hypothesis space** is the space of **linear predictors** \n",
    "\n",
    "$$ h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x} = \\sum_{i=1}^{d} w_{i} x_{i}.$$\n",
    "\n",
    "In contrast, the predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$ obtained from an ANN using weights $\\mathbf{w}$ is highly non-linear. However, the basic principle of learning a good predictor by adjusting the weight vector $\\mathbf{w}$ is the same for linear regression (linear predictors) and deep learning (maps represented by ANN).\n",
    "\n",
    "This notebook demonstrates how to find an optimal linear predictor by minimizing the average loss incurred over a training set. This average loss is also known as the **training error** of a predictor. By evaluating the training error for predictors with different weight vectors, we obtain an objective or **loss function** $f(\\mathbf{w})$. The loss function maps a weight vector to the training error incurred by the predictor map corresponding to that weight vector. \n",
    "\n",
    "We will detail a simple iterative algorithm which is called **gradient descent** (GD). Loosely speaking, GD finds an approximate minimizer of the training error by incrementally improving the current guess for the optimal predictor (weights) by moving into the opposite gradient direction. We will also discuss a slight variation of GD which is known as **stochastic gradient descent** (SGD). SGD is one of the most widely used optimization methods within deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function (see Sec. 1.1.5. of Course Book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning methods aim at finding a good choice for the weights (and bias) of an **artificial neural network (ANN)**. To measure how \"good\" a particular choice for the weights is, we need to define a loss function. For a given pair of predicted label value $\\hat{y}$ and true label value $y$, the loss function $L(y,\\hat{y})$ provides a measure for the error, or \"loss\", incurred in predicting the true label $y$ by $\\hat{y}$. We emphasize that the precise definition of the loss function is a design choice. In prinipcle, the deep learning engineer is free to define the loss function. \n",
    "\n",
    "However, some particular definitions for the loss function have proven useful in many applications. If the label values are numeric (like a temperature or a weight), then the squared error loss $L(y,\\hat{y})=(y-\\hat{y})^2$ is often a good choice for the loss function. If the label values are categories (like \"cat\" and \"dog\"), we might use  the \"0/1\" loss $L(y,\\hat{y})=1$ if and only if $y=\\hat{y}$ and $L(y,\\hat{y})=0$.\n",
    "\n",
    "Deep learning methods are fed with a large number of labeled data points \n",
    "\n",
    "$$\\big(\\mathbf{x}^{(1)},y^{(1)}\\big),\\ldots,\\big(\\mathbf{x}^{(m)},y^{(m)}\\big).$$\n",
    "\n",
    "To measure the quality of a particular choice for the weights $\\mathbf{w}$ of an ANN, we first compute the resulting predictions $\\hat{y}^{(i)}$ obtained when feeding the feature vectors $\\mathbf{x}^{(i)}$ into the ANN. \n",
    "Then we calculate the average loss (or \"training error\")\n",
    "\n",
    "$$ (1/m) \\big( L(y^{(1)},\\hat{y}^{(1)})+L(y^{(2)},\\hat{y}^{(2)})+\\ldots+L(y^{(m)},\\hat{y}^{(m)}) \\big).$$\n",
    "\n",
    "Note that the training error depends on the weights $\\mathbf{w}$ of the ANN via the predictions $\\hat{y}^{(i)}$. Indeed, the predictions $\\hat{y}^{(i)}=h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$ are obtained by applying the ANN with weights $\\mathbf{w}$ to the input feature vector $\\mathbf{x}^{(i)}$. By evaluating the training error for different choices for the weights, we obtain a **cost function** $f(\\mathbf{w})$ which guides the optimal choice for the weights $\\mathbf{w}$. \n",
    "\n",
    "The choice for the weights resulting in minimum training error are obtained by solving \n",
    "\n",
    "$$ \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w})$$.\n",
    "\n",
    "With a slight abuse of notation, we refer to the function $f(\\mathbf{w})$ also as the loss function. For a given choice for the weight vector $\\mathbf{w}$, the loss function $f(\\mathbf{w})$ provides a quality measure for this choice. It should be clear from context if we mean by **loss function** the training error $f(\\mathbf{w})$ (which is a function of a weight vector) or the loss $L(y,\\hat{y})$ itself (which is a function of a pair of label values). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error \n",
    "\n",
    "Maybe the most widely used loss function for applications involving numeric label values $y \\in \\mathbb{R}$ is the squared error loss \n",
    "\n",
    "$$L(y,\\hat{y}) = (\\underbrace{y- \\hat{y}}_{\\mbox{prediction error}})^{2}.$$\n",
    "\n",
    "We assess the quality of a predictor $\\hat{y} = h^{(\\mathbf{w})}(\\mathbf{x})$ by the average loss incurred over a set of labeled data points (the **training set**). For the squared error loss this average is referred to as the **mean squared error (MSE)** \n",
    "\n",
    "$$ f(\\mathbf{w}) = (1/m) \\big( \\big( y^{(1)}-\\hat{y}^{(1)}\\big)^{2}+\\big( y^{(2)}-\\hat{y}^{(2)}\\big)^{2}+\\ldots+\\big( y^{(m)}-\\hat{y}^{(m)}\\big)^{2} \\big). $$\n",
    "Note that the MSE on the right hands side depends on the weight vector $\\mathbf{w}$ via the predictions $\\hat{y}^{(i)}$ obtained by applying the predictor map $h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the loss $f(\\mathbf{w})$, viewed as a function of the weights $\\mathbf{w}$, depends on two components. First, it depends on how the predictor maps depends on the weights. Second, it depends on the choice for the loss function $L(y,\\hat{y})$ used to measure the loss incurred by predicting the true label value $y$ with the prediction $\\hat{y}$. \n",
    "\n",
    "The combination of linear predictor functions and the squared error loss $L(y,\\hat{y})=(y-\\hat{y})^{2}$ is very popular as they result in a [convex](https://en.wikipedia.org/wiki/Convex_function) and [differentiable](https://en.wikipedia.org/wiki/Differentiable_function) loss function $f(\\mathbf{w})$. A convex function has the attractive property that any local minimum is always also a [global minimum](https://en.wikipedia.org/wiki/Maxima_and_minima#/media/File:Extrema_example_original.svg). If a convex function is also differentiable, it can be minimized by a simple but powerful algorithm which is known as **gradient descent**. \n",
    "\n",
    "<img src=\"MSELinPred.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning methods use predictor maps represented by an ANN with tunable weights. In this case, the predictor depends non-linearly on the weights. As a result, we obtain (highly) non-convex loss landscapes. Below you can see examples of loss function landscapes of more complicated models (neural networks), which illustrates that finding a minimum of these loss functions is not a trivial task.\n",
    "\n",
    "<img src=\"NNloss.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "<center><a href=\"https://www.cs.umd.edu/~tomg/projects/landscapes/\">image source</a></center>\n",
    "<center><a href=\"https://arxiv.org/abs/1712.09913/\">original paper</a></center>\n",
    "\n",
    "Here you can find more examples of visualizations for loss functions obtained from representing a predictor map using ANN:\n",
    "\n",
    "[3D visualization of NN loss functions](http://www.telesens.co/loss-landscape-viz/viewer.html) \\\n",
    "[3D animation of NN loss functions](https://www.youtube.com/watch?time_continue=32&v=aq3oA6jSGro&feature=emb_logo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent \n",
    "\n",
    "We will now introduce a simple algorithm that allows to find (good approximations to) the optimal weight $\\mathbf{w}_{\\rm opt}$ for a predictor map $h^{(\\mathbf{w})}(\\mathbf{x})$. The optimum weight vector should result in the smallest possible loss  \n",
    "\n",
    "\\begin{align} \n",
    "f(\\mathbf{w}_{\\rm opt}) = \\min_{\\mathbf{w} \\in \\mathbb{R}^{d}} f(\\mathbf{w}) \\mbox{ with } f(\\mathbf{w})& = (1/m) \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)} \\big)^{2} \\nonumber \\\\ \n",
    "& =(1/m) \\sum_{i=1}^{m} (y^{(i)} - h^{(\\mathbf{w})}\\big(\\mathbf{x}^{(i)}\\big) \\big)^{2}. \\end{align}\n",
    "\n",
    "**Gradient descent (GD)** constructs a sequence of weight vectors $\\mathbf{w}^{(0)},\\mathbf{w}^{(1)},\\ldots$ such that the loss values $f\\big(\\mathbf{w}^{(0)}\\big),f\\big(\\mathbf{w}^{(0)}\\big),\\ldots$ tends toward the minimum loss. GD is an iterative algorithm that gradually improves the current guess (approximation) $\\mathbf{w}^{(k)}$ for the optimum weight vector.  \n",
    "\n",
    "There are many different strategies for choosing the first (or initial) guess $\\mathbf{w}^{(0)}$. One simple approach is to chose the initial weights randomly. Given the current weight vector $w^{(k)}$, how does GD know in which \"direction\" to go to find a better weight vector $\\mathbf{w}^{(k+1)}$?. Mathematics, or [calcululs](https://en.wikipedia.org/wiki/Differential_calculus) more precisely, tells us that this direction is precisely the opposite of the gradient $\\nabla f(\\mathbf{w})$. More precisely, for small step size, the steepest descent is towards the opposite direction of the [gradient](https://en.wikipedia.org/wiki/Derivative). We can think of GD as imitating a hiker who takes a sequence of (small) steps downhill. \n",
    "[read here](https://en.wikipedia.org/wiki/Gradient_descent).\n",
    "\n",
    "<img src=\"GradientHiker.jpeg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Given the downhill direction $- \\nabla f\\big(\\mathbf{w}^{(k)}\\big)$ at the current estimate $\\mathbf{w}^{(k)}$, we take a step \n",
    "\n",
    "$$\\mbox{(Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha}_{\\mbox{step size}} \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "\n",
    "Here, we used a tuning parameter $\\alpha>0$ which adjusts the step size for the step downlhill. We will refer to this parameter as the **learning rate**. This name is due to the fact that chosing a larger value for $\\alpha$ tends to speed up the progress of GD to reach the optimum weight vector. Thus, increasing the value of $\\alpha$ tends to speed up the learning of a good weight vector for a predictor map $h^{(\\mathbf{w})}$. \n",
    "\n",
    "The GD algorithm amounts to chosing a suitable value for the learning rate $\\alpha$ and initial guess $\\mathbf{w}^{(0)}$ and then repeating the gradient step for a sufficient number of iterations. One possible stopping criterion is to use a fixed number of iterations which might be dictated by constraints on processing duration we grant for GD (computing time costs money, [see here](https://aws.amazon.com/emr/pricing/)). \n",
    "\n",
    "Another option is to monitor the loss function and stop if consecutive iterates do not result in any significant decrease. Similarly, we could monitor the valiation loss which is obtained by applying the predictor map using the current GD iterate $\\mathbf{w}^{(k)}$ to validation data which is different form the training data used to define the training loss. \n",
    "\n",
    "A key challenge in the use of GD is to find a good choice for the learning rate $\\alpha$. If the learning rate is too small (left plot below), the GD steps make too litte progress and thus requires an excessive number of iterations to get close to the optimum weight vector. Conversely, if the learning rate is too high (right plot below), it is possible that GD iterates $\\mathbf{w}^{(k)}$ will \"overshoot\" the minimum and climb up the loss function on the other side of the minimum (GD diverges). \n",
    "\n",
    "<img src=\"lrate.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure below visualizes how GD adapts the weights $\\mathbf{w}$ of a linear predictor $h^{(\\mathbf{w})}(\\mathbf{x})= \\mathbf{w}^{T} \\mathbf{x}$ to better fit the labeled data points (left) resulting in a smaller MSE (right). Note that after around $100$ iterations, gradient descent found weight vectors resulting in an almost minimum MSE. The additional iterations (beyond $100$) are (in some sense) a waste of computation as they do not decrease the MSE significantly. \n",
    "\n",
    "![SegmentLocal](plainGD.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to gain some intuition about the functioning of GD, let us work out the gradient update for the special case of linear predictors maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. Here, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The gradient update of GD then becomes, in turn, \n",
    "\n",
    "\\begin{align} \n",
    "\\mathbf{w}^{(k+1)} & = \\mathbf{w}^{(k)} + (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big)\\nonumber \\\\ \n",
    " & = \\mathbf{w}^{(k)} + (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big( y^{(i)} - \\hat{y}^{(i)} \\big) \n",
    ". \\end{align}\n",
    "\n",
    "Note that the gradient update involves the computation of the predictions $\\hat{y}^{(i)} = \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)}$. We will refer to this evaluation of the predictor map also as a **forward pass**. After the forward pass, the weight vector $\\mathbf{w}^{(k)}$ is updated by a weighted combination of the feature vectors $\\mathbf{x}^{(i)}$. The weight for the $i$th feature vector $\\mathbf{x}^{(i)}$ is given by the prediction error $ \\big( y^{(i)} - \\hat{y}^{(i)} \\big)$ incurred by the current weight vector for that data point. Thus, the gradient update puts more emphasize (larger weight) on those data points $\\big(\\mathbf{x}^{(i)},y^{(i)}\\big)$ which are not well predicted using the current weight vector $\\mathbf{w}^{(k)}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent \n",
    "\n",
    "Despite its conceptual simplicity, GD might be difficult to use in ML applications involing massive amounts of data. Consider image classification where state-of-the art deep learning methods are trained on billions of images. The challenge in using GD for such big data applications is the computational complexity of computing the gradient $\\nabla f\\big( \\mathbf{w}^{(k)} \\big)$ of the loss function at the current estimate $\\mathbf{w}^{(k)}$. Let us have a closer look at the computation of the gradient for the special case of linear predictor maps $h^{(\\mathbf{w})}(\\mathbf{x}) = \\mathbf{w}^{T} \\mathbf{x}$. In this case, we can find a closed-form expression for the gradient: \n",
    "\n",
    "\\begin{align} \n",
    "\\nabla f\\big( \\mathbf{w}^{(k)} \\big)= - (2/m) \\sum_{i=1}^{m}\\mathbf{x}^{(i)} \\big(y^{(i)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(i)} \\big) \\big). \\end{align}\n",
    "\n",
    "The challenge is that the summation involves all $m$ data points that form the training set. Thus, we might need to sum over billions of data points which might be stored decentralized all over the internet. A single iteration of GD might simply take too long in such a setting. \n",
    "\n",
    "In order to avoid the computational burden of computing the gradient, stochastic GD (SGD) approximates the gradient \n",
    "by using only a small subset (a \"batch\") of training data points. SGD is obtained from GD by replacing the exact gradient step by a noisy gradient update: \n",
    "$$\\mbox{(Noisy Gradient Step)} \\quad \\underbrace{\\mathbf{w}^{(k+1)}}_{\\mbox{new guess}} = \\underbrace{\\mathbf{w}^{(k)}}_{\\mbox{current guess}} - \\underbrace{\\alpha^{(k)}}_{\\mbox{step size}}\\mathbf{g}^{(k)} \\mbox{ with } \\mathbf{g}^{(k)} \\approx \\nabla f\\big(\\mathbf{w}^{(k)}\\big).$$ \n",
    "Note that we now use a varying step-size $\\alpha^{(k)}$ that changes along the iterations. This is necessary in order to attenuate the noise in the gradient estimate $\\mathbf{g}^{(k)}$. \n",
    "\n",
    "The most basic variant of SGD uses a single randomly chosen data point $\\big(\\mathbf{x}^{(i)},y^{(i)}$ for computing the gradient estimate $\\mathbf{g}^{(k)}$. For the special case of linear predictor maps, we obtain \n",
    "$$ \\mathbf{g}^{(k)}   = 2 \\mathbf{x}^{(I)} \\big(y^{(I)} - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x}^{(I)} \\big) \\big).$$\n",
    "Note that the index $I$ of the data point is chosen randomly and indepdently for each new iteration $k$. Comparing this gradient estimate with the above formula for the exact gradient, we see that the SGD iteration does not require any summation over the training set. For large training sets this might yield a significant reduction in computational requirements for SGD compared to GD. \n",
    "\n",
    "Plain SGD and GD can be interpreted as spaces cases of **mini-batch SGD**. Mini-batch SGD does not use a single randomly chosen data point to compute the gradient estimate but rather uses several randomly chosen data points that form a batch $\\mathcal{S} = \\big\\{ \\big(\\mathbf{x}^{(i_{1})},y^{(i_{1})} \\big),\\ldots,\\big(\\mathbf{x}^{(i_{S})},y^{(i_{S})} \\big) \\big\\}$ of size $S$. For linear predictor maps, the gradient estimate is computed using the batch via \n",
    "$$ \\mathbf{g}^{(k)}   = 2 \\sum_{\\big(\\mathbf{x},y\\big) \\in \\mathcal{S}} \\mathbf{x} \\big(y - \\big( \\mathbf{w}^{(k)}\\big)^{T} \\mathbf{x} \\big) \\big).$$\n",
    "Each iteration of mini-batch SGD uses a different batch of $S$ different data points. When the algorithm pass through all batches, or through **entire dataset**, we say that algorithm ran for one **epoch**. For example, if the training data set consists of $100$ data points and we use a batch size of $S=10$, then one epoch requires $100/10 = 10$ iterations of mini-batch SGD. \n",
    "\n",
    "We can order the SGD algorithms discussed so far based on the **batch size** (number of samples per batch):\n",
    "\n",
    "- GD (batch size = size of dataset)\n",
    "- Mini-batch SGD (1 < batch size < whole dataset)\n",
    "- plain SGD (batch size = 1)\n",
    "\n",
    "The pros of using batches instead of the entire dataset is that it is computationally easier, therefore faster and cons - it requires more learning time (more epochs to run). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many machine learning and deep learning Python libraries, such as `sklearn` and `keras`, provide ready-to-use gradient based optimization algorithms. However, it is instructive to implement our own simplified mini-batch stochastic gradient descent (or mini-batch SGD) algorithm for learning purposes. In this simple case we have 100 data points (samples) which are described by only one feature `X` and have labels stored in `y`. \\\n",
    "We need to find the optimal linear predictor `y_pred = weight*X`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all needed Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate some dataset by using sklearn.dataset `make_regression` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset for regression problem\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=20, random_state=2) \n",
    "X = X.reshape(-1,)\n",
    "X = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used sklearn `preprocessing` module ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html#sklearn-preprocessing-scale)) to scale our features `X`. Learn [here](https://www.youtube.com/watch?v=r5E2X1JdHAU&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=20), why it is useful to normalize the data when applying the gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before implementing the SGD algorithm, we need to write a helper function `batch()` that divides the dataset into small batches. We will provide the feature matrix `X` and label vector `y` as an input to the function. We also need to define the parameter `batch_size` which is the number of data points used for a single batch. The function `batch()` is a [Python generator function](https://docs.python.org/3/howto/functional.html#generators), meaning that the function can be used in for-loops and will return batches sequentially, one-by-one. Before splitting the dataset into batches, we will randomly shuffle the data. This ensures that every time we call `batch()` we obtain a batch having the same statistical properties. Loosely speaking, by shuffling the dataset before selecting the batch makes the individual data points independent and identically distributed (\"i.i.d.\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(X,y,batch_size):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This is a function for creating minibatches of the dataset.\n",
    "    The yield statement suspends function’s execution and sends \n",
    "    a value back to the caller, but retains enough state to enable \n",
    "    function to resume where it is left off. \n",
    "    \n",
    "    '''\n",
    "    # check if the number of data points is equal in feature matrix X and label vector y\n",
    "    # if assertion fails return error message \"Number of data points are different in X and y\"\n",
    "    assert X.shape[0] == y.shape[0], \"Number of datapoints are different in X and y\"\n",
    "    \n",
    "    # shuffle data points \n",
    "    # permutation will randomly re-arrange the order of the numbers\n",
    "    # which will be used as indices to create X and y with data points in different order\n",
    "    p = np.random.permutation(len(y))\n",
    "    X_perm = X[p] \n",
    "    y_perm = y[p]\n",
    "    \n",
    "    # generate batches\n",
    "    for i in range(0,X.shape[0],batch_size):\n",
    "        yield (X_perm[i:i + batch_size], y_perm[i:i + batch_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we implement a single gradient step. It will be implemented in a `grad_descent()` function. This function has as its input parameters the feature matrix `X`, the label vector `y`, the initial guess for the weights and a learning rate. The function `grad_descent()` will:\n",
    "\n",
    "1. compute predictions for the data points in the bach, given the current weights\n",
    "2. compute MSE loss \n",
    "3. compute gradient of the loss function\n",
    "4. update the weights - change the weights values to the opposite direction from gradient `weight = weight - lrate*gradient`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientstep(X,y,weight,lrate):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    This is a function for performing palin gradient descent algorithm with MSE loss function.\n",
    "\n",
    "    squared error loss for a single data point:        \n",
    "      loss = (y - weight*x)**2\n",
    "    derivative w.r.t. to weight for a single data point:  \n",
    "      der_w = -2x*(y - weight*x)\n",
    "      \n",
    "    To compute the MSE and derivative of MSE - take an average of all data points.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # performing Gradient Descent:\n",
    "    \n",
    "    # 1. compute predictions, given the weight\n",
    "    y_hat = weight*X   \n",
    "    # 2. compute MSE loss\n",
    "    MSE = np.mean((y - y_hat)**2)\n",
    "    # 3. compute gradient of loss function\n",
    "    der_w = -2*np.mean(X*(y - y_hat))  \n",
    "    # 4. update the weights\n",
    "    weight = weight - lrate* der_w            \n",
    " \n",
    "    return weight, MSE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine all elements in `minibatchSGD()` function. The parameters of the function: \\\n",
    "`X` - features \\\n",
    "`y` - labels   \\\n",
    "`batch_size` - number of samples per batch \\\n",
    "`epoches` - how many times to iterate through the ENTIRE dataset \\\n",
    "`lrate` - step size or learning rate of gradient descent\n",
    "\n",
    "The function will return learnt weight after running the algorithm through entire dataset epoches times.\n",
    "In addition, the function will return list `loss` where the MSE loss values for all batches and epoches are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def minibatchSGD(X,y,batch_size,epochs,lrate):  \n",
    "    \n",
    "    # initialize weight randomly\n",
    "    weight = np.random.rand()    \n",
    "    # create list to store the loss values \n",
    "    loss = []\n",
    "     \n",
    "    for i in range(epochs):\n",
    "        # run gradient descent for each batch\n",
    "        for X_batch,y_batch in batch(X,y,batch_size):\n",
    "            weight, MSE = gradientstep(X_batch,y_batch,weight,lrate)\n",
    "            # store MSE loss of each batch of each epoch\n",
    "            loss.append(MSE)\n",
    "            \n",
    "        # one epoch is finished when the algorithm goes through ALL batches\n",
    "    return weight, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our SGD implementation and run the algorithm for:\n",
    "\n",
    "- batch sizes = 1 (one data point) **(\"true\" SGD)**\n",
    "- batch sizes = 10 **(mini-batch SGD)**\n",
    "- batch sizes = 100 (entire dataset) **(Batch GD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set epoches and learning rate\n",
    "epochs = 200\n",
    "lrate = 0.1\n",
    "\n",
    "# batch size 1\n",
    "model1 = minibatchSGD(X,y,1,epochs,lrate)\n",
    "# batch size 10\n",
    "model2 = minibatchSGD(X,y,10,epochs,lrate)\n",
    "# batch size 100\n",
    "model3 = minibatchSGD(X,y,100,epochs,lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `minibatchSGD()` returns weight and loss incurred during the training. Let's retrieve the loss values for plotting and print out learnt weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  200\n",
      "Iterations per epoch: 100, 10, 1,\n",
      "Total number of iterations:  20000 2000 200\n",
      "\n",
      "Weights: \n",
      "\n",
      "SGD with batch size=1 results in weight w=58.77 \n",
      "SGD with batch size =10 w=60.93 \n",
      "batch size=100 60.27\n"
     ]
    }
   ],
   "source": [
    "# history of the MSE loss inccured during learning\n",
    "batch_loss1 = model1[-1]\n",
    "batch_loss2 = model2[-1]\n",
    "batch_loss3 = model3[-1]\n",
    "\n",
    "# let's check that length of list `loss` is equal to\n",
    "# X.shape[0]/batch_size*epochs\n",
    "print(\"Epochs: \", epochs)\n",
    "print(\"Iterations per epoch: {:.0f}, {:.0f}, {:.0f},\".format(X.shape[0]/1, X.shape[0]/10, X.shape[0]/100))\n",
    "print(\"Total number of iterations: \",len(batch_loss1), len(batch_loss2), len(batch_loss3))\n",
    "\n",
    "# display weight learnt during the SGD\n",
    "print(\"\\nWeights: \\n\\nSGD with batch size=1 results in weight w={:.2f} \\nSGD with batch size =10 w={:.2f} \\nbatch size=100 {:.2f}\".format(model1[0], model2[0], model3[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the optimal weights learnt by our algorithm and the optimal weight calculated by sklearn `LinearRegression()` class. This class does not use iterative gradient-based algorithms, but rather calculating optimal weight analytically [learn more here](https://www.youtube.com/watch?v=B-Ks01zR4HY).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.27113167581487"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create linear regression model \n",
    "reg = LinearRegression(fit_intercept=False) \n",
    "# fit a linear regression model \n",
    "reg = reg.fit(X.reshape(-1,1), y)\n",
    "# print optimal coefficients\n",
    "reg.coef_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, our simple GD algorithm does not deviate too much from the `sklearn` implementation of linear regression. Note that the deviations are larger for smaller batch size. This is makes sense, as the gradient estimates are more accurate when we use more datapoints in a batch. \\\n",
    "It is useful to plot the loss values incurred during the training or learning. Let us plot the loss values for first 100 iterations (for first 100 batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1,3, sharey=True, figsize=(15,5))\n",
    "\n",
    "batch_loss_list = [batch_loss1, batch_loss2, batch_loss3]\n",
    "size_list       = [1,10,100] # batch size\n",
    "\n",
    "\n",
    "for ax, batch_loss, size in zip(axes, batch_loss_list, size_list):\n",
    "    ax.plot(np.arange(len(batch_loss[:100])), batch_loss[:100])\n",
    "    \n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    ax.set_title(\"batch size = \"+str(size), fontsize=18)\n",
    "    \n",
    "axes[0].set_xlabel('batch #', fontsize=18)\n",
    "axes[0].set_ylabel('Loss', fontsize=18)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure indicates that when using SGD with a small batch size, the loss is not decreasing monotoncially but somewhat randomly fluctuating around a long-term decreasing trend. This happens because the weight updates use \"noisy\" estimates of the gradient. The noisy estimate is calculated by an averaging process using the data points in the mini-batch. The smaller the mini-batch size, the fewer data points we use for computing the average. Thus, the gradient noise becomes stronger with smaller batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the animations illustrating the training process with SGD where batch size is all data points (upper panel) and where batch size is 10 data points (lower panel, the current batch marked with red color). In line with loss plots we created above, mini-batch SGD is more noisy than batch GD. Although, it seems that using plain batch GD is faster way to reach the minimum of the loss function, in practice when working with large datasets and thousands parameters (weights) in neural networks, this approach will be slower and more computationally expensive than mini-batch SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration of SGD iterations for batch size = 100 (batch covers entire dataset)** \n",
    "![SegmentLocal](minibatchGD1.gif \"segment\")\n",
    "**Illustration of SGD iterations for batch size = 10 datapoints**\n",
    "![SegmentLocal](minibatchGD2.gif \"segment\")\n",
    "During each iteration of SGD, 10 data points are randomly selected to constitute a batch. This batch is used \n",
    "to compute the gradient estimate. The data points in the batch are shown in red. Note that during \n",
    "each iteration, a different set of 10 data points is chosen for the batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of Gradient-Based Optimization Algorithms\n",
    "\n",
    "Beside plain GD or mini-batch SGD, many deep learning methods use somewhat more advanced variants of gradient-based algorithms or optimizers ([list of optimizers available in deep learning Python library Keras](https://keras.io/api/optimizers/)). Some of the most most known are SGD with momentum, RMSprop and Adam. \n",
    "\n",
    "Much like GD and SGD, these algorithms use gradients of the loss function $f(\\mathbf{w})$ to find weights $\\mathbf{w}$ such that the predictor $h^{(\\mathbf{w})}$ achieves (nearly) minimum loss. These variants differ in how they use (or \"interpret\") the gradient information to find the fastest route towards the minimum. In some cases these variants can find good weight vectors significantly faster (using fewer iterations) compared to mini-batch SGD. \n",
    "\n",
    "The animation below compares the \"routes\" taken by different optimizers to find a minimum of the [six-hump camel](https://www.sfu.ca/~ssurjano/camel6.html) function. \n",
    "\n",
    "![SegmentLocal](camel3D.gif \"segment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "We have discussed the basic idea of using gradients of loss functions to iteratively improve the parameter values (weights) in a predictor map. Gradient based methods turn out to be the perfect tool for deep learning mehtods in several aspects. Somehwat surprisingly, stochastic gradient descent (and its variants) are able to quickly find values for the parameters (weights) of a (deep) artificial neural network such that it performs well on new data points which are different from the training data. Moreover, mini-batch SGD requires only to have enough working memory (\"RAM\") to store the current batch (subset) of training data points instead of the entire dataset (which might be billions of high-resolution images). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommended reading:**\n",
    "\n",
    "- Deep Learning with Python F.Chollet, [chapter 2.4](https://livebook.manning.com/book/deep-learning-with-python/chapter-2/193)\n",
    "\n",
    "**Additional materials:**\n",
    "\n",
    "*derivatives*\n",
    "\n",
    "- https://www.mathsisfun.com/calculus/derivatives-introduction.html\n",
    "- (chapters 3-4) https://openstax.org/books/calculus-volume-1/pages/3-1-defining-the-derivative#27277\n",
    "\n",
    "*gradient descent algorithm*\n",
    "\n",
    "Video \n",
    "- Andrew Ng, https://www.youtube.com/watch?v=F6GSRDoB-Cg \n",
    "- StatQuest, https://www.youtube.com/watch?v=sDv4f4s2SB8\n",
    "- 3Blue1Brown, https://www.youtube.com/watch?v=IHZwWFHWa-w\n",
    "\n",
    "Books\n",
    "\n",
    "- (intermediate) Machine Learning: Basic Principles (chapter 5) https://arxiv.org/pdf/1805.05052.pdf\n",
    "- (advanced) Deep Learning Book https://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "Why do we need a loss function in machine learning?\n",
    "\n",
    "1. To transform the data into a computer friendly format. \n",
    "2. To speed up the learning process of a ML method. \n",
    "3. To measure the quality of a predictor obtained from a ML method. \n",
    "4. To reduce the memory requirements of a ML method. \n",
    "\n",
    "### Q2\n",
    "\n",
    "Consider the average loss, or training error, incurred by a predictor with weights $\\mathbf{w}$ on a set of labeled data points, or trainin set. Which of the following statemets are correct?\n",
    "\n",
    "1. The training error depends only on the labels $y^{(1)},\\ldots,y^{(m)}$. \n",
    "2. The training error depends on the features, labels and weights. \n",
    "3. The training error does not depend on the features $\\mathbf{x}^{(1)},\\ldots,\\mathbf{x}^{(m)}$.\n",
    "4. The training error does not depend on the labels. \n",
    "5. The training error does not depend on the weights. \n",
    "\n",
    "### Q3\n",
    "\n",
    "Gradient descent is a ...\n",
    "\n",
    "1. method to avoid overfitting\n",
    "2. iterative algorithm for a loss function minimization\n",
    "3. method to compute the optimal parameters in one step\n",
    "4. method to divide the data into batches\n",
    "\n",
    "\n",
    "### Q4\n",
    "\n",
    "Consider the update rule of GD: $\\mathbf{w}^{(k+1)} = \\mathbf{w}^{(k)} - \\alpha \\nabla f(\\mathbf{w}^{(k)})$ with learning rate $\\alpha >0$. Here, $f(\\mathbf{w})$ is the MSE incurred by a linear predictor $h(\\mathbf{x}) = \\mathbf{w})^{T} \\mathbf{x}$ on a training set $\\big(\\mathbf{x}^{(1)},y^{(m)}\\big),\\ldots,\\big(\\mathbf{x}^{(1)},y^{(m)}\\big)$. Which of the following statements are correct?\n",
    "\n",
    "1. The GD iterates $\\mathbf{w}^{(1)}, \\mathbf{w}^{(2)},\\ldots,$ will always converge.  \n",
    "2. The GD iterates will converge if we choose the learning rate $\\alpha$ sufficiently small. \n",
    "3. The GD iterates will converge if we choose the learning rate sufficiently large. \n",
    "4. The GD iterates will converge only for certain training sets. \n",
    "\n",
    "### Q5\n",
    "\n",
    "Compare to GD, SGD ...\n",
    "\n",
    "1. has reduced computaional cost \n",
    "2. makes the learning path less noisy\n",
    "3. requires less training data\n",
    "4. always converges faster, i.e., requires fewer iterations to deliver a weight vector that is within a prescribed distance to the optimum weight vector. \n",
    "\n",
    "### Q6\n",
    "\n",
    "Which of the following statements are correct? \n",
    "\n",
    "1. batch size is always equal to 1 \n",
    "2. epochs is the total number of iterations SGD performes\n",
    "3. during the gradient descent we change the weights values to the same direction as gradient\n",
    "4. epochs is the number of times SGD goes through entire dataset\n",
    "\n",
    "\n",
    "### Q7\n",
    "\n",
    "Consider a dataset with 4800 data points. If we run mini-bath SGD with a batch size of 320 and for 100 epochs, what is the total number of iterations used in SGD?\n",
    "\n",
    "1. 160000\n",
    "2. 15\n",
    "3. 7500\n",
    "4. 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "174.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
